{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7e7a1ab8-2599-417d-9a65-25ef07f3a786",
      "metadata": {
        "id": "7e7a1ab8-2599-417d-9a65-25ef07f3a786"
      },
      "source": [
        "# Lab | Web Scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce8882fc-4815-4567-92fa-b4816358ba7d",
      "metadata": {
        "id": "ce8882fc-4815-4567-92fa-b4816358ba7d"
      },
      "source": [
        "Welcome to the IMDb Web Scraping Adventure Lab!\n",
        "\n",
        "**Objective**\n",
        "\n",
        "In this lab, we will embark on a mission to unearth valuable insights from the vast sea of data available on IMDb, one of the largest online databases of movie, TV, and celebrity information. As budding data scientists and business analysts, you have been tasked to scrape a specific subset of data from IMDb to assist film production companies in understanding the landscape of highly-rated movies in a defined time period. Your insights will potentially influence the making of the next netflix movie!\n",
        "\n",
        "**Background**\n",
        "\n",
        "In a world where data has become the new currency, businesses are leveraging big data to make informed decisions that drive success and profitability. The entertainment industry, being no exception, utilizes data analytics to comprehend market trends, audience preferences, and the performance of films based on various parameters such as director, genre, stars involved, etc. IMDb stands as a goldmine of such data, offering intricate details of almost every movie ever made.\n",
        "\n",
        "**Task**\n",
        "\n",
        "Your task is to create a Python script using `BeautifulSoup` and `pandas` to scrape IMDb movie data based on user ratings and release dates. This script should be able to filter movies with ratings above a certain threshold and within a specified date range.\n",
        "\n",
        "**Expected Outcome**\n",
        "\n",
        "- A function named `scrape_imdb` that takes four parameters: `title_type`,`user_rating`, `start_date`, and `end_date`.\n",
        "- The function should return a DataFrame with the following columns:\n",
        "  - **Movie Nr**: The number representing the movieâ€™s position in the list.\n",
        "  - **Title**: The title of the movie.\n",
        "  - **Year**: The year the movie was released.\n",
        "  - **Rating**: The IMDb rating of the movie.\n",
        "  - **Runtime (min)**: The duration of the movie in minutes.\n",
        "  - **Genre**: The genre of the movie.\n",
        "  - **Description**: A brief description of the movie.\n",
        "  - **Director**: The director of the movie.\n",
        "  - **Stars**: The main stars of the movie.\n",
        "  - **Votes**: The number of votes the movie received.\n",
        "  - **Gross ($M)**: The gross earnings of the movie in millions of USD.\n",
        "\n",
        "You will execute this script to scrape data for movies with the Title Type `Feature Film` that have a user rating of `7.5 and above` and were released between `January 1, 1990, and December 31, 1992`.\n",
        "\n",
        "Remember to experiment with different title types, dates and ratings to ensure your code is versatile and can handle various searches effectively!\n",
        "\n",
        "**Resources**\n",
        "\n",
        "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
        "- [Pandas Documentation](https://pandas.pydata.org/pandas-docs/stable/index.html)\n",
        "- [IMDb Advanced Search](https://www.imdb.com/search/title/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3519921d-5890-445b-9a33-934ed8ee378c",
      "metadata": {
        "id": "3519921d-5890-445b-9a33-934ed8ee378c"
      },
      "source": [
        "**Hint**\n",
        "\n",
        "Your first mission is to familiarize yourself with the IMDb advanced search page. Head over to [IMDb advanced search](https://www.imdb.com/search/title/) and input the following parameters, keeping all other fields to their default values or blank:\n",
        "\n",
        "- **Title Type**: Feature film\n",
        "- **Release date**: From 1990 to 1992 (Note: You don't need to specify the day and month)\n",
        "- **User Rating**: 7.5 to -\n",
        "\n",
        "Upon searching, you'll land on a page showcasing a list of movies, each displaying vital details such as the title, release year, and crew information. Your task is to scrape this treasure trove of data.\n",
        "\n",
        "Carefully examine the resulting URL and construct your own URL to include all the necessary parameters for filtering the movies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25a83a0d-a742-49f6-985e-e27887cbf922",
      "metadata": {
        "id": "25a83a0d-a742-49f6-985e-e27887cbf922"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "**Best of luck! Immerse yourself in the world of movies and may the data be with you!**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b75cf0d-9afa-4eec-a9e2-befeac68b2a0",
      "metadata": {
        "id": "7b75cf0d-9afa-4eec-a9e2-befeac68b2a0"
      },
      "source": [
        "**Important note**:\n",
        "\n",
        "In the fast-changing online world, websites often get updates and make changes. When you try this lab, the IMDb website might be different from what we expect.\n",
        "\n",
        "If you run into problems because of these changes, like new rules or things that stop you from getting data, don't worry! Instead, get creative.\n",
        "\n",
        "You can choose another website that interests you and is good for scraping data. Websites like Wikipedia or The New York Times are good options. The main goal is still the same: get useful data and learn how to scrape it from a website that you find interesting. It's a chance to practice your web scraping skills and explore a source of information you like."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e7c762c",
      "metadata": {},
      "source": [
        "# My Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85a4b286",
      "metadata": {},
      "source": [
        "Since the directions for this lab applied to a webpage version of IMDB that was out of date, I was directed by my instructor to conduct some webscraping on whatever piqued my interest, so long as I could learn or glean some helpful information from my work. I decided, therefore, to apply webscraping to see if a personal project of mine could be made easier and more effective. \n",
        "\n",
        "I've been trying to figure out a way to divide up the study of the different books of the Standard Works of the Church of Jesus Christ of Latter-day Saints into chunks of daily reading that are both manageable and consistent. I have tried doing succh breakdowns by verse, but have found that there is quite a wide variation in verse lengths, even in the same chapters or on the same pages. That inconsistency can lead to major differences in the amount of time a reader will spend on one chunk of daily reading. I often will read 17 verses one day and find it only spans about a page, and then read 17 verses the next day and find that the same number of verses spans two pages because the verses are longer on the second day than on the first. \n",
        "\n",
        "So, I wanted to use webscraping to see if I could more effectively breakup those daily chunks by the number of lines to read. This level of nuance will provide for a much higher level of consistency from day to day as I seek to come closer to Christ by studying in His word, while also helping me not be so stressed out when I find that one day's reading is twice as long as the day before's despite being the same number of verses.|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40359eee-9cd7-4884-bfa4-83344c222305",
      "metadata": {
        "id": "40359eee-9cd7-4884-bfa4-83344c222305"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import time\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import re\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b99f19ee",
      "metadata": {},
      "source": [
        "I started off by setting my working directory and navigating to the Doctrine and Covenants webpage found on the main website of the Church, and set up a response request to start looking at the html code of the page. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "183c5917",
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir(\"D:\\Faith and Religion Stuff\\Come, Follow Me Breakdowns\")\n",
        "url = 'https://www.churchofjesuschrist.org/study/scriptures/dc-testament/dc/1?lang=eng'\n",
        "\n",
        "response = requests.get(url)\n",
        "response\n",
        "response.content\n",
        "response.headers\n",
        "print(response.headers['Content-Type'])\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "type(soup)\n",
        "content = soup.prettify()  # This formats the HTML in a readable way\n",
        "print(content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "801fede5",
      "metadata": {},
      "source": [
        "Using beautiful soup allowed me to identify a few important pieces of information. First off, when looking at any given chapter of scripture, I found verse tags that gave me information about each verse and how it is displayed. Then, when looking at the contents page of the Doctrine and Covenants, I found that there were href links to each section, and in each of the < a > lines there was also a text title associated with the tag < a clclass=\"sc-omeqik-0 ewktus\">. Having that information, I knew that I could get the information I needed. \n",
        "\n",
        "I got a lot of help from ChatGPT on this, but I was able to work with it download and access a google driver and use Selenium to do a lot of the same webscraping things that BeautifulSoup can do. \n",
        "\n",
        "After installing the driver and Selenium, I had to establish the path for accessing the driver, test to see if it was working, set options for Chrome, and create the service function I'd be using. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3982eab",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the path to the chromedriver executable\n",
        "chrome_driver_dir = r'C:\\\\Users\\\\bfran\\\\Downloads\\\\ChromeDriver\\\\chromedriver-win64'\n",
        "chrome_driver_path = os.path.join(chrome_driver_dir, 'chromedriver.exe')\n",
        "\n",
        "# Verify that the path is correct and the file exists\n",
        "print(f\"Checking if the chromedriver exists at: {chrome_driver_path}\")\n",
        "if not os.path.isfile(chrome_driver_path):\n",
        "    raise FileNotFoundError(f\"The chromedriver executable was not found at the specified path: {chrome_driver_path}\")\n",
        "else:\n",
        "    print(\"Chromedriver found!\")\n",
        "\n",
        "# Set up the headless browser options\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--disable-gpu\")\n",
        "chrome_options.add_argument(\"--window-size=1920x1080\")\n",
        "\n",
        "# Set up the Chrome service\n",
        "service = Service(chrome_driver_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8508f632",
      "metadata": {},
      "source": [
        "From there, it was a lot of poking around and tweaking and adjusting. My first order of business was to see if I could even access the text of a verse. After that, I relied on ChatGPT's guidance to get the font-size and line heights to estimate the number of lines. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f24628b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the Chrome WebDriver\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "# Use driver to get url\n",
        "driver.get(url)\n",
        "\n",
        "# Allow some time for rendering\n",
        "time.sleep(2)\n",
        "\n",
        "# Find the element containing the text\n",
        "container = driver.find_element(By.CSS_SELECTOR, '.verse')\n",
        "\n",
        "# Get the text of the element\n",
        "text = container.text\n",
        "\n",
        "# Get the bounding rectangle of the element\n",
        "rect = container.rect\n",
        "\n",
        "# Print the information\n",
        "print(f\"Container dimensions: width={rect['width']} height={rect['height']}\")\n",
        "print(f\"Text content: {text}\")\n",
        "\n",
        "# Check if the content height suggests line breaks\n",
        "verse_element = driver.find_element(By.CSS_SELECTOR, '.verse')\n",
        "font_size = driver.execute_script(\"return window.getComputedStyle(arguments[0]).getPropertyValue('font-size');\", verse_element)\n",
        "\n",
        "# Calculate line height\n",
        "line_height_str = driver.execute_script(\"return window.getComputedStyle(arguments[0]).getPropertyValue('line-height');\", verse_element)\n",
        "line_height_numeric = int(re.search(r'\\d+', line_height_str).group())  # Extract numeric value from string\n",
        "num_lines = rect['height'] // line_height_numeric\n",
        "\n",
        "print(f\"Estimated number of lines: {num_lines}\")\n",
        "\n",
        "# Close the browser\n",
        "driver.quit()\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90c93dc4",
      "metadata": {},
      "source": [
        "Once I established that I could do that with one verse, and that the estimate was accurate, I could do it for a whole chapter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77c83430",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the Chrome WebDriver\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "# Run the driver\n",
        "driver.get(url)\n",
        "\n",
        "# Allow some time for rendering\n",
        "time.sleep(2)\n",
        "\n",
        "# Find all elements containing the text\n",
        "verses = driver.find_elements(By.CSS_SELECTOR, '.verse')\n",
        "\n",
        "# Iterate over each verse element\n",
        "for verse in verses:\n",
        "    # Get the text of the element\n",
        "    text = verse.text\n",
        "\n",
        "    # Get the bounding rectangle of the element\n",
        "    rect = verse.rect\n",
        "\n",
        "    # Print the information\n",
        "    print(f\"Container dimensions: width={rect['width']} height={rect['height']}\")\n",
        "    print(f\"Text content: {text}\")\n",
        "\n",
        "    # Calculate font size\n",
        "    font_size = driver.execute_script(\"return window.getComputedStyle(arguments[0]).getPropertyValue('font-size');\", verse)\n",
        "\n",
        "    # Calculate line height\n",
        "    line_height_str = driver.execute_script(\"return window.getComputedStyle(arguments[0]).getPropertyValue('line-height');\", verse)\n",
        "    line_height_numeric = int(re.search(r'\\d+', line_height_str).group())  # Extract numeric value from string\n",
        "\n",
        "    # Calculate number of lines\n",
        "    num_lines = rect['height'] // line_height_numeric\n",
        "\n",
        "    print(f\"Estimated number of lines: {num_lines}\")\n",
        "    print(\"-\" * 50)  # Separator for clarity\n",
        "\n",
        "# Close the browser\n",
        "driver.quit()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43bf7306",
      "metadata": {},
      "source": [
        "After getting that to work, I knew I could move on to just saving the information collected to a dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09df373c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the Chrome WebDriver\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "# Run the driver\n",
        "driver.get(url)\n",
        "\n",
        "# Find all elements containing the text\n",
        "verses = driver.find_elements(By.CSS_SELECTOR, '.verse')\n",
        "\n",
        "# Initialize a list to store data dictionaries\n",
        "data_list = []\n",
        "\n",
        "# Iterate over each verse element\n",
        "for verse in verses:\n",
        "    # Get the text of the element\n",
        "    text = verse.text\n",
        "\n",
        "    # Extract verse number (assuming it's in the format \"1 \", \"2 \", etc.)\n",
        "    verse_number = text.split(' ')[0]  # Assuming verse number is at the start of text\n",
        "    \n",
        "    # Get the bounding rectangle of the element\n",
        "    rect = verse.rect\n",
        "\n",
        "    # Calculate line height\n",
        "    line_height_str = driver.execute_script(\"return window.getComputedStyle(arguments[0]).getPropertyValue('line-height');\", verse)\n",
        "    line_height_numeric = int(re.search(r'\\d+', line_height_str).group())  # Extract numeric value from string\n",
        "\n",
        "    # Calculate number of lines\n",
        "    num_lines = rect['height'] // line_height_numeric\n",
        "\n",
        "    # Append data dictionary to list\n",
        "    data_list.append({\n",
        "        'Verse Number': verse_number,\n",
        "        'Num Lines': num_lines\n",
        "    })\n",
        "\n",
        "# Convert list of dictionaries to DataFrame\n",
        "df = pd.DataFrame(data_list)\n",
        "\n",
        "# Print the DataFrame (optional)\n",
        "print(df)\n",
        "\n",
        "# Close the browser\n",
        "driver.quit()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b69ce081",
      "metadata": {},
      "source": [
        "And once that was working, I knew I could just write a function that would take the input of a url to a chapter of scripture and then use that url to get the line lengths of each verse. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b37c0628",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_verse_lines(url):\n",
        "    \"\"\"\n",
        "    This function takes a URL as input and returns the number of lines in the HTML content of all the verses in a given chapter of Holy Scripture and stores the verses and line counts in a pandas dataframe.\n",
        "    \"\"\"\n",
        "    # Import needed packages\n",
        "    import pandas as pd\n",
        "    import requests\n",
        "    import os\n",
        "    import time\n",
        "    from selenium import webdriver\n",
        "    from selenium.webdriver.chrome.service import Service\n",
        "    from selenium.webdriver.common.by import By\n",
        "    from selenium.webdriver.chrome.options import Options\n",
        "    import re\n",
        "\n",
        "    # Define the path to the chromedriver executable\n",
        "    chrome_driver_dir = r'C:\\\\Users\\\\bfran\\\\Downloads\\\\ChromeDriver\\\\chromedriver-win64'\n",
        "    chrome_driver_path = os.path.join(chrome_driver_dir, 'chromedriver.exe')\n",
        "\n",
        "    # Set up the headless browser options\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(\"--window-size=1920x1080\")\n",
        "\n",
        "    # Set up the Chrome service\n",
        "    service = Service(chrome_driver_path)    \n",
        "    \n",
        "    # Initialize the Chrome WebDriver\n",
        "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "    # Run the driver\n",
        "    driver.get(url)\n",
        "\n",
        "    # Find all elements containing the text\n",
        "    verses = driver.find_elements(By.CSS_SELECTOR, '.verse')\n",
        "\n",
        "    # Initialize a list to store data dictionaries\n",
        "    data_list = []\n",
        "\n",
        "    # Iterate over each verse element\n",
        "    for verse in verses:\n",
        "        # Get the text of the element\n",
        "        text = verse.text\n",
        "\n",
        "        # Extract verse number (assuming it's in the format \"1 \", \"2 \", etc.)\n",
        "        verse_number = text.split(' ')[0]  # Assuming verse number is at the start of text\n",
        "    \n",
        "        # Get the bounding rectangle of the element\n",
        "        rect = verse.rect\n",
        "\n",
        "        # Calculate line height\n",
        "        line_height_str = driver.execute_script(\"return window.getComputedStyle(arguments[0]).getPropertyValue('line-height');\", verse)\n",
        "        line_height_numeric = int(re.search(r'\\d+', line_height_str).group())  # Extract numeric value from string\n",
        "\n",
        "        # Calculate number of lines\n",
        "        num_lines = rect['height'] // line_height_numeric\n",
        "\n",
        "        # Append data dictionary to list\n",
        "        data_list.append({\n",
        "            'Verse Number': verse_number,\n",
        "            'Num Lines': num_lines\n",
        "        })\n",
        "\n",
        "    # Convert list of dictionaries to DataFrame\n",
        "    df = pd.DataFrame(data_list)\n",
        "\n",
        "    # Close the browser\n",
        "    driver.quit()\n",
        "\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a3c6abf",
      "metadata": {},
      "source": [
        "Having successfully done that, I could have called it a day, but instead I decided that if I could get that information, surely I could also use webscraping to get a dataframe of links that I could feed into that function using an iteration loop. \n",
        "\n",
        "Again, I relied on ChatGPT to walk me through the process and help me gather the right information. \n",
        "\n",
        "I started with just getting the links. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a371dfef",
      "metadata": {},
      "outputs": [],
      "source": [
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "try:\n",
        "    # Navigate to the page with your elements\n",
        "    driver.get('https://www.churchofjesuschrist.org/study/scriptures/dc-testament?lang=eng')\n",
        "\n",
        "    # Find all elements with the specified class name\n",
        "    elements = driver.find_elements(By.CLASS_NAME, 'sc-omeqik-0')\n",
        "\n",
        "    # Initialize a list to store href values\n",
        "    href_list = []\n",
        "    title_list = []\n",
        "\n",
        "    # Iterate over each element and extract the href attribute\n",
        "    for element in elements:\n",
        "        href = element.get_attribute('href')\n",
        "        href_list.append(href)\n",
        "\n",
        "    # Create a DataFrame to store the href values\n",
        "    urls_df = pd.DataFrame({'Href': href_list})\n",
        "\n",
        "    # Print the DataFrame (optional)\n",
        "    print(urls_df)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "finally:\n",
        "    # Close the browser\n",
        "    driver.quit()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e97e1438",
      "metadata": {},
      "source": [
        "But the links would do no good at all if I didn't have the section titles, because I realized I'd also want to save each section or chapter's verse lengths as separate dataframes that I could export as csv files. So, I spent a long time trying to get that worked out. Well, actually, I spent a long time trying to work out nonexistent issues because I didn't realize the output of my attempts to get titles and links was a scrollable cell, so I thought I kept just getting links. \n",
        "\n",
        "Some additional things added to this cell's code are things that I had to remove - there were more titles than href links, so I had to go in and remove the titles that didn't have links. Additionally, Official Declaration 1 and 2 have different links, but the same title, so I had to duplicate that title. This process was just a whole lot of trial and error. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c77e9b99",
      "metadata": {},
      "outputs": [],
      "source": [
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "try:\n",
        "    # Navigate to the page with your elements\n",
        "    driver.get('https://www.churchofjesuschrist.org/study/scriptures/dc-testament?lang=eng')\n",
        "\n",
        "    # Find all elements with the specified classes\n",
        "    href_elements = driver.find_elements(By.CSS_SELECTOR, 'a.sc-omeqik-0.ewktus')\n",
        "    title_elements = driver.find_elements(By.CSS_SELECTOR, 'p.title')\n",
        "\n",
        "    # Debugging: Print lengths of elements found\n",
        "    print(f\"Number of href elements: {len(href_elements)}\")\n",
        "    print(f\"Number of title elements: {len(title_elements)}\")\n",
        "\n",
        "    # Skip the very first title element\n",
        "    if title_elements:\n",
        "        title_elements = title_elements[1:145]\n",
        "\n",
        "    # Delete the 4th title element (index 3)\n",
        "    if len(title_elements) > 3:\n",
        "        del title_elements[3]\n",
        "\n",
        "    # Duplicate the last title element\n",
        "    if title_elements:\n",
        "        title_elements.append(title_elements[-1])\n",
        "\n",
        "    # Delete the 4th title element (index 3)\n",
        "    if len(href_elements) > 142:\n",
        "        del href_elements[142]\n",
        "\n",
        "    # Initialize lists to store href and title values\n",
        "    href_list = [element.get_attribute('href') for element in href_elements]\n",
        "    title_list = [element.text for element in title_elements]\n",
        "\n",
        "    # Create a list of dictionaries to store matched data\n",
        "    matched_data = []\n",
        "    min_length = min(len(href_list), len(title_list))\n",
        "\n",
        "    # Match hrefs and titles based on the minimum length\n",
        "    for i in range(min_length):\n",
        "        matched_data.append({'Title': title_list[i], 'Href': href_list[i]})\n",
        "\n",
        "    # Create a DataFrame from matched data\n",
        "    dc_links_df = pd.DataFrame(matched_data)\n",
        "\n",
        "    # Set Pandas display options to show full content of 'Href' column\n",
        "    pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "    # Print the DataFrame to verify the 'Href' column contents\n",
        "    print(dc_links_df)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "finally:\n",
        "    # Close the browser\n",
        "    driver.quit()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99636c55",
      "metadata": {},
      "source": [
        "Once it was working, though, I could subset the dataframe into the introductory stuff, the actual sections of the Doctrine and Covenants, and the Official Declarations. I had to do this because, while the sections of the Doctrine and Covenants are \"versified,\" the introductory materials and Official Declarations are not, so I'll have to get those line lengths later. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76d13d6b",
      "metadata": {},
      "outputs": [],
      "source": [
        "dc_sections = dc_links_df.iloc[4:142]\n",
        "dc_intro = dc_links_df.iloc[:3]\n",
        "dc_ods = dc_links_df[142:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df096364",
      "metadata": {},
      "source": [
        "After subsetting the data all I had to do was establish my iteration loop, and the result was a nice set of individual csv files that I'll be able to use SQL queries to join and access so that I can make my own daily breakdowns later. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a730b7ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the directory path where you want to save the CSV files\n",
        "dir_path = r'D:\\\\Faith and Religion Stuff\\\\Come, Follow Me Breakdowns\\\\D&C Sections Verse Lines'\n",
        "\n",
        "# Ensure the directory exists, if not create it\n",
        "os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "for index, row in dc_sections.iterrows():\n",
        "    title = row['Title']\n",
        "    link = row['Href']\n",
        "\n",
        "    verse_lines_df = get_verse_lines(link)\n",
        "\n",
        "    if verse_lines_df is not None and not verse_lines_df.empty:\n",
        "        csv_filename = f'{title.replace(\" \",\"_\").lower()}_verse_lines.csv'\n",
        "        full_path = os.path.join(dir_path, csv_filename)\n",
        "        \n",
        "        # Debugging: Print full path to ensure it's correct\n",
        "        print(f'Saving to: {full_path}')\n",
        "\n",
        "        verse_lines_df.to_csv(full_path, index=False)\n",
        "\n",
        "        print(f'CSV file for \"{title}\" saved successfully as {csv_filename}.')\n",
        "    else:\n",
        "        print(f'No data for \"{title}\", skipping CSV creation.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ab1217a-df9d-4a2b-a32c-2c5f1eecd52a",
      "metadata": {
        "id": "9ab1217a-df9d-4a2b-a32c-2c5f1eecd52a"
      },
      "source": [
        "## BONUS\n",
        "\n",
        "The search results span multiple pages, housing a total of 631 movies in our example with each page displaying 50 movies at most. To scrape data seamlessly from all pages, you'll need to dive deep into the structure of the URLs generated with each \"Next\" click.\n",
        "\n",
        "Take a close look at the following URLs:\n",
        "- First page:\n",
        "  ```\n",
        "  https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,\n",
        "  ```\n",
        "- Second page:\n",
        "  ```\n",
        "  https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=51&ref_=adv_nxt\n",
        "  ```\n",
        "- Third page:\n",
        "  ```\n",
        "  https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=101&ref_=adv_nxt\n",
        "  ```\n",
        "\n",
        "You should notice a pattern. There is a `start` parameter incrementing by 50 with each page, paired with a constant `ref_` parameter holding the value \"adv_nxt\".\n",
        "\n",
        "Modify your script so it's capable of iterating over all available pages to fetch data on all the 631 movies (631 is the total number of movies in the proposed example)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21ac4fc0-a12b-4a00-9266-2020166f0dea",
      "metadata": {
        "id": "21ac4fc0-a12b-4a00-9266-2020166f0dea"
      },
      "outputs": [],
      "source": [
        "# Your solution goes here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
